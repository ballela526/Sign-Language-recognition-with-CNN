# Sign Language Recognition using CNN

This project is a real-time sign language recognition system that uses Convolutional Neural Networks (CNN) to identify American Sign Language (ASL) gestures and convert them into spoken output. The system is designed for mobile devices and includes a user-friendly Android app with voice feedback.

Project Highlights:

- Used CNN to classify ASL hand gestures accurately.
- Trained the model using TensorFlow on an ASL dataset from Kaggle.
- Converted the model to TensorFlow Lite for deployment on Android.
- Integrated OpenCV for capturing live video and preprocessing frames.
- Developed an Android application in Android Studio using Java.
- The app allows users to add gestures, read them aloud, clear the input, and insert spaces.

Tools and Technologies:

- Python, Java
- TensorFlow, Keras, TensorFlow Lite
- OpenCV
- Android Studio


Future Work:

- Extend support for Indian Sign Language (ISL)
- Improve accuracy under different lighting and hand positions
- Add multilingual voice output support
- Expand gesture coverage beyond alphabet letters


Published in IEEE under the title "Deep Learning for Sign Language Interpretation with CNN"  
Conference: ICSTSDG 2024  
DOI: 10.1109/ICSTSDG61998.2024.11026679
